import requests
import pandas as pd
from datetime import datetime
import json
import sys
import os

# === IMPORTA TOKEN DE AUTH ===
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
from auth.config import TOKEN

# === CONFIGURA√á√ïES DA API ===
URL = "https://apitotvsmoda.bhan.com.br/api/totvsmoda/person/v2/legal-entities/search"

headers = {
    "Authorization": f"Bearer {TOKEN}",
    "Content-Type": "application/json"
}

payload = {
    "filter": {
        "change": {
            "startDate": "2023-10-01T00:00:00Z",
            "endDate": "2025-10-26T23:59:59Z",
            "inAddress": True,
            "inPhone": True,
            "inObservation": True,
            "inPerson": True
        },
        "isCustomer": True,
        "isSupplier": True,
        "personCodeList": [110000002]
    },
    "option": {
        "branchStaticDataList": [0]
    },
    "expand": "addresses,phones,emails,observations,contacts,partners,socialNetworks",
    "order": "code",
    "page": 1,
    "pageSize": 100
}

print("üöÄ Iniciando consulta de entidades jur√≠dicas...")
print(f"üì¶ Payload enviado:\n{json.dumps(payload, indent=2)}")

# === REQUISI√á√ÉO POST ===
try:
    response = requests.post(URL, headers=headers, json=payload, timeout=60)
except requests.exceptions.RequestException as e:
    print(f"‚ùå Erro na conex√£o: {e}")
    sys.exit(1)

print(f"üì° Status HTTP: {response.status_code}")
if response.status_code != 200:
    print("‚ùå Erro na resposta da API:")
    print(response.text)
    sys.exit(1)

# === TRATAMENTO DO JSON ===
try:
    data = response.json()
except requests.exceptions.JSONDecodeError:
    print("‚ùå Erro ao decodificar JSON da resposta.")
    sys.exit(1)

# === SALVA DEBUG ===
debug_file = f"debug_legal_entities_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
with open(debug_file, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
print(f"üíæ Debug salvo em: {debug_file}")

# === INSPE√á√ÉO DE CHAVES PRINCIPAIS ===
print("\nüîç Estrutura principal da resposta:")
for key, value in data.items():
    tipo = type(value).__name__
    tamanho = len(value) if isinstance(value, (list, dict)) else "-"
    print(f"   - {key} ({tipo}) tamanho: {tamanho}")
print("-" * 60)

# === EXTRA√á√ÉO DE DADOS PRINCIPAIS ===
items = data.get("items", [])
if items:
    df_main = pd.json_normalize(items, sep="_", max_level=1)
    print(f"‚úÖ {len(df_main)} registros encontrados na lista principal.")
else:
    df_main = pd.DataFrame()
    print("‚ö†Ô∏è Nenhum registro encontrado na lista principal.")

# === EXTRA√á√ÉO DE LISTAS ANINHADAS ===
nested_fields = ["addresses", "phones", "emails", "observations", "customerObservations",
                 "additionalFields", "classifications", "references", "relateds", "partners",
                 "contacts", "socialNetworks", "representatives"]

nested_dfs = {}

for field in nested_fields:
    nested_list = []
    for item in items:
        person_code = item.get("code")
        for entry in item.get(field) or []:  # garante que seja iter√°vel
            entry["personCode"] = person_code
            nested_list.append(entry)
    if nested_list:
        nested_dfs[field] = pd.DataFrame(nested_list)
        print(f"üìù {field}: {len(nested_dfs[field])} registros extra√≠dos.")
    else:
        nested_dfs[field] = pd.DataFrame()
        print(f"‚ö†Ô∏è {field}: nenhum registro encontrado.")

# === EXPORTA√á√ÉO PARA EXCEL ===
excel_file = f"legal_entities_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
try:
    with pd.ExcelWriter(excel_file, engine="xlsxwriter") as writer:
        if not df_main.empty:
            df_main.to_excel(writer, index=False, sheet_name="LegalEntities")
        for key, df_nested in nested_dfs.items():
            if not df_nested.empty:
                df_nested.to_excel(writer, index=False, sheet_name=key)
    print(f"‚úÖ Relat√≥rio Excel gerado com sucesso: {excel_file}")
except Exception as e:
    print(f"‚ùå Erro ao exportar para Excel: {e}")

print("üèÅ Execu√ß√£o finalizada com sucesso.")
